{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YP_iCpjvvo9",
        "outputId": "4812f899-8c30-4aef-f7cc-ef0810af0ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: openai 0.27.8\n",
            "Uninstalling openai-0.27.8:\n",
            "  Would remove:\n",
            "    /usr/local/bin/openai\n",
            "    /usr/local/lib/python3.10/dist-packages/openai-0.27.8.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/openai/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled openai-0.27.8\n",
            "Collecting openai==0.27.8\n",
            "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "#!pip install --upgrade openai\n",
        "!pip uninstall openai\n",
        "!pip install openai==0.27.8\n",
        "\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "5_GoaavqwD_F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = {'turbo':\"gpt-3.5-turbo\",'davinci':\"text-davinci-003\"}"
      ],
      "metadata": {
        "id": "Nqobkqj8wKr_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_keywords = 20\n",
        "idea_brief = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaxlCLeKwRjw",
        "outputId": "149d16dd-d118-40e4-8b41-43d35ab4e001"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Low code platform will be driven by LLMS in future\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'''\n",
        "    Generate an SEO optimized outline for the blog writing and {num_keywords} relevant keywords based on the following idea brief:{idea_brief}.\n",
        "\n",
        "    The output must be in the following format and nothing else should be generated,\n",
        "\n",
        "    Outline: Generated outline here with headings and sub-headings.\n",
        "\n",
        "    Keywords: Generated keywords here, each separated by a numbered bullet.\n",
        "'''"
      ],
      "metadata": {
        "id": "fGtMMVMvwo3h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "p19Uszdrwtqv",
        "outputId": "e64b53a7-3d66-4408-c8eb-98081948a8a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    Generate an SEO optimized outline for the blog writing and 20 relevant keywords based on the following idea brief:Low code platform will be driven by LLMS in future.\\n\\n    The output must be in the following format and nothing else should be generated,\\n    \\n    Outline: Generated outline here with headings and sub-headings.\\n    \\n    Keywords: Generated keywords here, each separated by a numbered bullet.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "\n",
        "# response = openai.Completion.create(\n",
        "\n",
        "#     engine=GPT_MODEL['davinci'],\n",
        "\n",
        "#     prompt=prompt,\n",
        "\n",
        "#     max_tokens = 3600,\n",
        "\n",
        "#     n=1,\n",
        "\n",
        "#     temperature=0\n",
        "\n",
        "# )\n",
        "\n",
        "# print(response.choices[0].text.strip())\n",
        "# print()"
      ],
      "metadata": {
        "id": "JUqnI-67wyeh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "messages=[\n",
        "            {'role': 'system', 'content': 'You provide relevant keywords and outline for an idea brief.'},\n",
        "            {'role': 'user', 'content': prompt},\n",
        "        ]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "\n",
        "    model=GPT_MODEL['turbo'],\n",
        "\n",
        "    messages=messages,\n",
        "\n",
        "    max_tokens = 3600,\n",
        "\n",
        "    n=1,\n",
        "\n",
        "    temperature=0\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content.strip())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfeqV-XRw3_B",
        "outputId": "525ffe2e-41ac-4888-8254-1b8f03363c27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outline:\n",
            "1. Introduction\n",
            "    - Definition of low code platform\n",
            "    - Overview of LLMS (Low-Code/Low-Modeling Systems)\n",
            "2. The Rise of Low Code Platforms\n",
            "    - Explanation of the growing popularity of low code platforms\n",
            "    - Benefits of using low code platforms\n",
            "3. LLMS and the Future of Low Code Platforms\n",
            "    - Introduction to LLMS (Low-Level Modeling Systems)\n",
            "    - How LLMS enhances low code platforms\n",
            "4. Advantages of LLMS in Low Code Platforms\n",
            "    - Increased customization and flexibility\n",
            "    - Improved performance and scalability\n",
            "    - Streamlined development process\n",
            "5. Challenges and Limitations of LLMS in Low Code Platforms\n",
            "    - Complexity of LLMS implementation\n",
            "    - Potential security risks\n",
            "    - Learning curve for developers\n",
            "6. Use Cases of LLMS in Low Code Platforms\n",
            "    - Case studies of successful LLMS integration\n",
            "    - Industries benefiting from LLMS in low code platforms\n",
            "7. Future Trends and Predictions for LLMS in Low Code Platforms\n",
            "    - Integration of AI and machine learning in LLMS\n",
            "    - Expansion of LLMS capabilities\n",
            "8. Conclusion\n",
            "    - Recap of LLMS and its impact on low code platforms\n",
            "    - Final thoughts on the future of LLMS in the industry\n",
            "\n",
            "Keywords:\n",
            "1. Low code platform\n",
            "2. LLMS\n",
            "3. Low-Code/Low-Modeling Systems\n",
            "4. Low code development\n",
            "5. Low code software\n",
            "6. Low code tools\n",
            "7. Low code benefits\n",
            "8. Low code platforms advantages\n",
            "9. LLMS implementation\n",
            "10. Customization in low code platforms\n",
            "11. Flexibility in low code development\n",
            "12. Performance in low code platforms\n",
            "13. Scalability in low code software\n",
            "14. Streamlined development process\n",
            "15. Challenges of LLMS\n",
            "16. Security risks in low code platforms\n",
            "17. Learning curve for LLMS\n",
            "18. LLMS use cases\n",
            "19. AI in LLMS\n",
            "20. Future of LLMS in low code platforms\n",
            "\n",
            "CPU times: user 168 ms, sys: 11.6 ms, total: 180 ms\n",
            "Wall time: 10.6 s\n"
          ]
        }
      ]
    }
  ]
}